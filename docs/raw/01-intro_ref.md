# 开场与引入

## AI 能力与边界——第一堂课
- 本课聚焦于当前最主流的 AI 类型——大语言模型（LLM）及其衍生能力
- 目标：帮助学员理解 AI 的基础原理、能力圈、边界
- 输出：在项目中能判断任务是否在 AI 的能力范围内

## AI 到底能做什么？
- 文本生成：写作、摘要、翻译、问答
- 代码生成与调试
- 多模态创作：图像、视频、音频
- 数据分析与自动化任务
- 也有做不到的：长期记忆、绝对真实的事实、强因果推理

## 本堂课目标
- 了解 LLM 的本质与工作原理
- 掌握推理能力、参数规模、多模态等核心概念
- 熟悉当前前沿模型的能力与边界
- 理解上下文、Token、温度、幻觉等关键限制

---

# 大语言模型基础

## 什么是 AI（聚焦 LLM）
- AI 范围很广，包括规则系统、传统机器学习、深度学习等
- 我们今天讲的主要是“大语言模型”——一种基于 Transformer 架构的生成式模型
- LLM 代表：GPT-5、Claude、Gemini、LLaMA 等

## LLM 的通俗定义
- 本质：超大规模的“文字预测机器”
- 输入文字，模型逐词预测最可能的下一个词
- 像超级强化版的手机输入法，但有更大的知识面和模式识别能力

## LLM 如何工作
- 输入转 Token → 编码（Attention 处理上下文） → 解码生成 Token
- 核心机制：预测下一个 Token 的概率分布
- 随机采样和概率控制让输出有多样性

## 推理能力的差别
- 有的模型几乎不推理，只模仿常见答案
- 推理型模型会按步骤分解问题
- 推理需要专门的训练数据和方法

## 推理能力的培养方式
- 高质量推理过程数据（思维链 CoT）
- 人类反馈（RLHF）或 AI 反馈（RLAIF）
- 工具调用辅助推理（计算器、搜索、代码执行）

## 参数（Billion）含义
- 模型的“旋钮”数量，控制预测输出
- 3B = 30 亿参数，30B = 300 亿，100B = 1000 亿
- 参数多 → 潜力大 → 成本和延迟也高

## 参数与能力的关系
- 大参数 ≠ 一定更好，训练质量很关键
- 小模型优势：成本低、速度快、垂直领域可更精准
- 案例：Mistral 7B、DeepSeek-Coder 6.7B、Med-PaLM 2

---

# 模型类型与多模态

## 单模态 vs 多模态
- 单模态：只处理文字（GPT-3.5、早期 LLaMA）
- 多模态：能处理多种输入（文字+图片+音频等）
- 多模态让模型“有眼睛”，但理解依然基于模式匹配

## 多模态的实现
- 视觉编码器（ViT、CLIP）将图片转为向量
- 与语言模型融合训练，建立模态间映射
- 训练数据：大规模的图片-文字对、视频-文字对

## 多模态的价值与边界
- 优势：图文理解、OCR、视觉问答、图像生成
- 局限：无法像人类一样做完整因果推理
- 成本更高，训练难度大

## 生成图片/视频的模型是不是 LLM
- 不是，生成视觉内容的模型（Flux、Veo 3）多用扩散模型
- LLM 输出文字，视觉模型输出像素
- 多模态 AI 里，LLM 负责理解需求，视觉模型负责生成

## 多模态趋势
- 从纯文本 → 图文 → 图文音视频融合
- 单一大模型内部整合多种模态能力
- 未来更多应用会直接融合多模态交互

## 多模态协作流程
- 用户 → LLM 解析任务 → 调用视觉/音频模型 → LLM 整合输出
- 类似“AI 团队协作”：语言专家 + 视觉艺术家 + 视频导演

---

# 能力与边界案例

## GPT-5 的能力与边界
- 能力：推理更强、知识更全、长上下文支持
- 边界：仍有幻觉、无长期记忆、复杂推理仍会错
- 本质是“即时专家”，非全知全能 AGI

## Google Veo 3 的能力与边界
- 能力：8 秒高保真视频生成、原生音频
- 边界：时长限制、成功率不稳定、动作控制精度有限
- 适合短促视觉表达，不替代长视频制作

## Flux 1.1 Pro Ultra 的能力与边界
- 能力：4M 像素高分辨率、细节丰富、艺术风格
- 边界：复杂结构易出错、逻辑矛盾 prompt 失败、跨图一致性差

## 前沿 TTS / 语音克隆的能力与边界
- 能力：多语言、情感语音、几秒样本即可克隆
- 边界：长段情绪保持不稳定、极端情绪不自然、实时性依赖硬件

## 能力圈与边界总结图
- 四象限：文本 / 图像 / 视频 / 音频
- 每象限标出“当前上限”和“主要短板”

## 上下文（Context）限制
- 窗口大小限制了模型一次能记住的信息量
- 例子：32k tokens ≈ 中篇小说
- 超长输入会遗忘早期信息

## Token 概念
- Token ≈ 英文 4 字符 ≈ 中文 2~3 个字
- 影响速度、成本、上下文长度
- 控制 token 使用能优化性能和价格

## 温度（Temperature）参数
- 高温度 → 创造性高、随机性大
- 低温度 → 输出稳定、确定性强
- 按任务类型调整：写诗 vs 写代码

## 幻觉（Hallucination）现象与防范方法
- 成因：生成概率机制导致虚构
- 防范：加检索、限制领域、工具验证

---

# 技术与实用技巧

## 训练 vs 推理
- 训练：让模型学习模式（高成本）
- 推理：用模型生成结果（日常使用）
- 微调（Fine-tuning）与提示词工程（Prompt Engineering）区别

## 工具调用（Function Calling）
- 模型可调用外部工具执行任务
- 常见：搜索引擎、计算器、数据库
- 是实现多模态与自动化的基础

## 小模型在垂直领域的应用
- 医疗：Med-PaLM 2
- 法律：Lawyer LLaMA
- 代码：DeepSeek-Coder、StarCoder

## 本课总结与课后任务
- 总结：LLM 的基础、能力圈、边界与限制
- 课后：选择一个模型，用 prompt 挑战它的边界并分享结果
